1、纽约地图非常大，节点多，但是十分稀疏，很多蚂蚁可能无法找到从起点到达终点的通路。

2、蚂蚁的初始化出发点都在起点，蚂蚁的初始化基本上不具备随机性。

一些可能的改进方式：
1、将启发示因子由  当前节点到待选节点的距离的倒数   改进为   待选节点到终点的距离的倒数
因为我们所追求的不完全是总体上距离最小，而是尽可能地向寻找到目标，所以需要更多的向目标靠近


2、建立网络社团结构，从而实现对大规模网络的分层化设计。
主要方法为找出区域枢纽点 可参考孙兵的那一篇学位论文

1.14
枢纽点就算找好了，但是如何确定哪些枢纽点之间应该存在连边呢？

改进蚁群算法的启示式因子的幂，因为一开始都不一定能够找到路径，不妨更加贪心一些，减少一些随机性。
随机性的好处在于能够更好地获得全局最优，但是在工程上，能够找到一条路径就已经是比较好的结果了。


1.17
完成了多因素多目标优化的加权统一
建立了多个因素的图网络，然后进行合并，由用户选择权重向量

1.18
使用networkx可以很快地求出两点之间的最短距离。
可以快速实现对图的预处理。找出区域枢纽点之间的最短路，然后进行储存，等需要时可以直接进行查询。


对于如何建立区域枢纽点之间的连边，
初步想法是，对于某一区域枢纽点，选择空间矩离它最近的20个枢纽点，然后分别求其与这20个区域枢纽点之间的最短路径，视为该枢纽点与周围枢纽点形成一条连边，权重即为最短路径的距离。
如果某一条最短路径途径了某一枢纽点，则不把该最短路径作为连边。


在区域枢纽点和连边建立好之后，即可以认为是高一级图网络建好了。
之后就可以直接进行子图划分。
子图划分的想法是，直接对非区域枢纽点，按照其空间距离哪个区域枢纽点最近，就将其划分到该枢纽点的区域内。

整个分层优化的算法流程即为，
1、先是查询起点和终点分别属于哪一个枢纽点的范围内。
2、分别使用蚁群算法，找到起点到其枢纽点a的最短路径，终点到其枢纽点b的最短路径。
3、在高一级图网络中，使用蚁群算法，找到枢纽点a到枢纽点b的最短路径。
4、再细化低一级图网络的路径查询。
5、再考虑引入多目标优化后的处理结果。


1.19
完成对区域枢纽点子图的建立，枢纽点子间最短路径的存储。
对其它非枢纽点，进行了子图划分。找到其对应的最近枢纽点。

1.20
gap一天

1.21
将蚁群算法原本的代码，更改了一下数据初始化格式等等，大大缩短搜索时间，减小内存占用。

蚁群算法为什么会慢那么多，
因为其存在概率接受，所以常常会把路径引导入那些并不是很优的路径中去，可能还会走进死胡同。
而且由于每一只蚂蚁的出发点都是起点，而不是随机分散在图中的其它节点上，所以并行运算的优势无法体现出来。
相当于每一只蚂蚁一直在重复前面蚂蚁的工作。除非前面的蚂蚁已经找到路径，否则依然是碰运气。
路径短一点的话还好，能够在较短的时间内碰出来。但是路径一旦很长，就十分困难了。第一只找到路径的蚂蚁总是迟迟无法出现。

1.22
完成分层算法下，多次蚁群算法实现近似最短路径的寻优。

1.25
在windows以及linux下配置git，然后将代码clone到虚拟机的master上，使用ipython的方式进行调试

1.26
学习和研究spark在本地模式下的运行原理。
学习如何使用RDD的方式进行分布式程序编写。

广播变量为什么无法设邻接表为广播变量啊！

1.27
好像是类对象必须满足可Serializable， 可序列化？

应该需要满足一定的条件，才可以序列化。

在python中使用networkx这个类所定义的图，可以转化为广播变量
可以使用这个类对蚁群算法的代码进行改写。

ant类好像转化为RDD时也会出问题，应该也是不可序列化的原因。
到底要怎么创建类才可以可序列化啊？

1.30
使用nx对蚁群算法进行了改写

2.14
成功实现将ant类转化为RDD。具体的方法，不过是将ant类中所用到的图对象（如distance_nx_graph，以及pheromone_nx_graph），作为形参传入。

但是还是无法实现map的分工，具体原因未知，可以尝试一下将自定义类分文件封装。
因为尝试了对更标准的类包所定义的对象（networkx）进行map分工，发现是可以的。
但是对自定义的类进行map分工却报错了。

2.15
尝试一下将自定义类分文件封装，发现真的可以诶
倒是没有报错了，但是就是一直跑不完一次迭代。

先尝试将Ant类分文件再import进来之后，非map方式运行，是否会报错。

2.16
成功实现对蚂蚁进行map分工搜索路径。完成一次迭代。
但多次运行程序后，似乎会多次累积RDD，以至于最后内存溢出。
如何让使用后的RDD自行结束，并释放出内存呢？

2.17
为什么老是内存溢出？
为什么运行迭代一次之后无法运行第二次？？第二次就总是无法结束运行，一直转不出来。
内存就这么小这么小吗？实在不理解！！！

把蚂蚁数量再次调小，就可以了。。。。。。调为10个。
可能确实是蚂蚁数量过多，导致内存溢出。但是很奇怪，总是第一次能正常运行，第二次就不行了

调正50只蚂蚁也是可以的。

然后就考虑更新信息素的问题了。


2.21
能够成功运行多次了，但是还是偶尔会出现内存溢出的情况。
而且搜索的速度特别慢，
原因未知，有可能是本地模式下运算资源不够的原因，也有可能是集群之间通信的原因。

本地模式下，可以运行，但运行速度较慢，且偶尔内存溢出。

yarn模式下，还暂时运行不了。格式化与HDFS相关的配置之后，可以成功启动。
但是运行算法时依然报错，无法找到classAnt类。

standalone模式下，依然报错无法找到classAnt类。

暂未解决classAnt类问题

2.22
最好不要在开启全虚拟机的同时，打开代理。
内存会爆掉。


加入一句sc.addPyFile("hdfs://master:9000/ant/classAnt.py")之后，
成功解决classAnt类报错问题，成功在standalone模式下运行成功。
但是内存真的会爆掉诶，这而且是在把蚂蚁数量设定为10的情况下跑的。

为什么会占据那么多的磁盘空间？ 
运行时
/dev/sda1        19G   18G  266M   99% /
关闭后
/dev/sda1        19G   14G  4.2G   76% /

master会参与计算吗？
不会参与。

2.23
在yarn模式下也成功运行。
只有master的磁盘空间在逐渐填充，而node的磁盘空间没有变化。
有可能是一些临时文件。/tmp/目录下会出现很多临时文件
要不再增大一下磁盘的容量？

改一下内存分配。
内存：master10G，worker每一个6G*3
磁盘容量：master分配为50G，worker每一个20G
处理器：每一个都分到两个cpu核。

感觉分布式的优势可以更多体现在，
每一次迭代时，
仅仅本地运行，无法承载太多的蚂蚁，而在分布式运行时，可以承载更多的蚂蚁，因而搜索得更快。
计算机集群，相当于扩展了内存，以及计算能力。

可以考虑把master的性能相对降低一些，然后比较仅仅在master上运行，和在集群上运行，有什么区别。

开起多个虚拟机时，处理器会重复使用吗
应该会重复占据线程。
https://zhidao.baidu.com/question/695004029939431804.html

将蚂蚁数量调整为30只，不会内存溢出。

但是好像每多跑一轮，就会多减少一些内存。但是也是动态变化的，会回升上去。

现在yarn模式下，能够保持较长时间迭代而不报错，磁盘空间留给临时文件的空间也足够多。
而且好像到一定程度，就又自行清理掉了。好像是满20G。

但是就是跑得很慢，感觉比单机状态下慢多了。。。。。大约20s完成一次stage
collect需要5s左右，而update信息素图需要3s左右，每一次update重新广播则需要13s左右

单机运行，每一次迭代只需要0.015s左右。
个人认为，主要是通讯时间占据了大量的时长。

为什么仅仅30只蚂蚁就会占据那么多的内存，
但是本机运行的情况下并不会占据很多内存？
本机运行时甚至可以把蚂蚁数量设得更大。。。

2.24
感觉可能是对计算资源的利用不够合理，学习一下如何分配内存和CPU。

2.28
将实例数改为3，
每一个executor的核由1改为2
发现每一个worker的内存又多占了1.5km的样子，但是搜索速度没有什么变化，

将每一个executor的核改回1
看看把蚂蚁数量改为50，即增加切片的数量，会不会导致内存溢出。
不会，且搜索时间也没有明显差别。

继续增加蚂蚁数量为100，发现内存溢出了。在spark配置中，给各节点各个executor分配更多的内存，还是内存溢出。

调整并行度？

对于一只蚂蚁，一定要让它找到一条路径才算结束，如果没有找到，就再找一次，直到找到为止。
这样对于一次迭代而言，肯定时长会长很多。然后再体现分布式的优势。

于是迭代一次的时间变为2484秒。

3.1
使用3cores，然后每个cores1024m内存，spark分布式计算，第一轮迭代的时间为990s。显著快于单机模式下。
而且基本上就是三倍的差别。说明三个cores可以并行三只蚂蚁的计算。

使用3cores，每个core512m内存，跑了1100s左右，比之前要慢一些。

再使用6cores，每个cores512m内存

3.8
记录一些可以较好到达的起终点，且spark加速效果明显的
297-1000

3.9
分层处理再加上spark分布式，发现老是进行第二次的时候内存溢出
有可能是蚂蚁数量过多导致。把蚂蚁数量改为30后可行。
30只蚂蚁的情况下，起终点为2000和145210
使用分层+spark：
时间：99+57+21+20+15+13
距离：1261283+887+38317

只使用分层：
时间：304+313+2.6+2.55+0.02+0.01
距离：48993+1144034+887
只使用spark：
无法在可预期的时间内搜索到
均不使用：
无法在可预期的时间内搜索到

可以明显看出，spark还是很快的，分层优化效果显著。

3.13
准备把notebook格式的代码整理为脚本模式，然后可以更方便地进行测试。
把一般的蚁群算法整理为脚本模式，把分层优化下的蚁群算法整理为脚本模式。

3.14
把spark下的蚁群算法整理为脚本模式。


3.15
spark-submit --master spark://master:7077 --deploy-mode client --executor-memory 1024m --deploy-mode client --total-executor-cores 3 map_view_nx_spark.py

3.16
考虑能不能找到一些样例，然后说明我的方法比传统的djsktra之类的更好。
找到一个样例，2961——145210，分层优化下比djsktra更好，速度更快。有的时候也并不需要最优解。

3.17
有的时候开代理还会影响github的上传。

2000-224103体现spark效果
20600-224103体现分层效果
54963-224103体现分层效果
75500 23789体现分层效果
157534 157800（pivot）体现spark效果，但是有点慢就是说
297 1000体现spark效果
174786 175455(pivot)体现spark效果
187800 187935(pivot)体现spark效果。
起终点交换，搜索难度会不同。
12014 12056(pivot)体现spark效果
247700 247380（pivot）体现spark效果
227881（pivot） 227941体现spark效果
1456 2000体现spark效果（有点慢）
6698 7510体现spark效果（有点慢）
146950 145699 体现spark效果
